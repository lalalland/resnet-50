{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose,GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\n",
    "from keras.layers import Input, add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, filters=64, n_block=3, kernel_size=(3, 3), activation='relu'):\n",
    "    skip = []\n",
    "    for i in range(n_block):\n",
    "        print('*********************encoder {}******************'.format(i+1))\n",
    "        print('input {}'.format(x.shape))\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        print('conv 1 {}'.format(x.shape))\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        print('conv 2 {}'.format(x.shape))\n",
    "        skip.append(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "        print('Max {}'.format(x.shape))\n",
    "        print('**************************************************************************')\n",
    "    return x, skip\n",
    "\n",
    "\n",
    "def bottleneck(x, filters_bottleneck, mode='cascade', depth=6,\n",
    "               kernel_size=(3, 3), activation='relu'):\n",
    "    dilated_layers = []\n",
    "    print('*********************************middle*********************************')\n",
    "    print('input {}'.format(x.shape))\n",
    "    if mode == 'cascade':  # used in the competition\n",
    "        print('cascade dilatation')\n",
    "        for i in range(depth):\n",
    "            \n",
    "            x = Conv2D(filters_bottleneck, kernel_size,\n",
    "                       activation=activation, padding='same', dilation_rate=2**i)(x)\n",
    "            dilated_layers.append(x)\n",
    "            print('conv dilatation {} {}'.format(i,x.shape))\n",
    "        return add(dilated_layers)\n",
    "    elif mode == 'parallel':  # Like \"Atrous Spatial Pyramid Pooling\"\n",
    "        print('parallel dilatation')\n",
    "        for i in range(depth):\n",
    "            dilated_layers.append(\n",
    "                Conv2D(filters_bottleneck, kernel_size,\n",
    "                       activation=activation, padding='same', dilation_rate=2**i)(x)\n",
    "            )\n",
    "            print('conv dilatation {} {}'.format(i,Conv2D(filters_bottleneck, kernel_size,\n",
    "                       activation=activation, padding='same', dilation_rate=2**i)(x).shape))\n",
    "        return add(dilated_layers)\n",
    "\n",
    "\n",
    "def decoder(x, skip, filters, n_block=3, kernel_size=(3, 3), activation='relu'):\n",
    "    \n",
    "    \n",
    "    for i in reversed(range(n_block)):\n",
    "        print('*********************************decoder{}*********************************'.format(i+1))\n",
    "        print('input {}'.format(x.shape))\n",
    "        x = UpSampling2D(size=(2, 2))(x)\n",
    "        print('upsampling {}'.format(x.shape))\n",
    "        print(filters * 2**i, kernel_size,)\n",
    "        if i == 3:\n",
    "            x = Conv2D(filters * 2**i, kernel_size, activation=None,strides=(2, 2), padding='valid')(x)\n",
    "            x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        else:\n",
    "            x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        print('conv 1 {}'.format(x.shape))\n",
    "        print('skip {}'.format(skip[i].shape))\n",
    "        x = concatenate([skip[i], x])\n",
    "        print('concat {}'.format(x.shape))\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        print('conv2 {}'.format(x.shape))\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        print('conv3 {}'.format(x.shape))\n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dilated_unet(\n",
    "        input_shape=(1920, 1280, 3),\n",
    "        mode='cascade',\n",
    "        filters=44,\n",
    "        n_block=4,\n",
    "        lr=0.0001,\n",
    "        loss='bce_dice_loss',\n",
    "        n_class=1\n",
    "):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    enc, skip = encoder(inputs, filters, n_block)\n",
    "    bottle = bottleneck(enc, filters_bottleneck=filters * 2**n_block, mode=mode)\n",
    "    dec = decoder(bottle, skip, filters, n_block)\n",
    "    classify = Conv2D(n_class, (1, 1), activation='sigmoid')(dec)\n",
    "    print('last {}'.format(classify.shape))\n",
    "    model = Model(inputs=inputs, outputs=classify)\n",
    "#     model.compile(optimizer=RMSprop(lr), loss=loss, metrics=[dice_coef])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************encoder 1******************\n",
      "input (?, 1024, 1024, 3)\n",
      "conv 1 (?, 1024, 1024, 32)\n",
      "conv 2 (?, 1024, 1024, 32)\n",
      "Max (?, 512, 512, 32)\n",
      "**************************************************************************\n",
      "*********************encoder 2******************\n",
      "input (?, 512, 512, 32)\n",
      "conv 1 (?, 512, 512, 64)\n",
      "conv 2 (?, 512, 512, 64)\n",
      "Max (?, 256, 256, 64)\n",
      "**************************************************************************\n",
      "*********************encoder 3******************\n",
      "input (?, 256, 256, 64)\n",
      "conv 1 (?, 256, 256, 128)\n",
      "conv 2 (?, 256, 256, 128)\n",
      "Max (?, 128, 128, 128)\n",
      "**************************************************************************\n",
      "*********************************middle*********************************\n",
      "input (?, 128, 128, 128)\n",
      "cascade dilatation\n",
      "conv dilatation 0 (?, 128, 128, 256)\n",
      "conv dilatation 1 (?, 128, 128, 256)\n",
      "conv dilatation 2 (?, 128, 128, 256)\n",
      "conv dilatation 3 (?, 128, 128, 256)\n",
      "conv dilatation 4 (?, 128, 128, 256)\n",
      "conv dilatation 5 (?, 128, 128, 256)\n",
      "*********************************decoder3*********************************\n",
      "input (?, 128, 128, 256)\n",
      "upsampling (?, 256, 256, 256)\n",
      "conv 1 (?, 256, 256, 128)\n",
      "skip (?, 256, 256, 128)\n",
      "concat (?, 256, 256, 256)\n",
      "conv2 (?, 256, 256, 128)\n",
      "conv3 (?, 256, 256, 128)\n",
      "*********************************decoder2*********************************\n",
      "input (?, 256, 256, 128)\n",
      "upsampling (?, 512, 512, 128)\n",
      "conv 1 (?, 512, 512, 64)\n",
      "skip (?, 512, 512, 64)\n",
      "concat (?, 512, 512, 128)\n",
      "conv2 (?, 512, 512, 64)\n",
      "conv3 (?, 512, 512, 64)\n",
      "*********************************decoder1*********************************\n",
      "input (?, 512, 512, 64)\n",
      "upsampling (?, 1024, 1024, 64)\n",
      "conv 1 (?, 1024, 1024, 32)\n",
      "skip (?, 1024, 1024, 32)\n",
      "concat (?, 1024, 1024, 64)\n",
      "conv2 (?, 1024, 1024, 32)\n",
      "conv3 (?, 1024, 1024, 32)\n",
      "last (?, 1024, 1024, 1)\n"
     ]
    }
   ],
   "source": [
    "model = get_dilated_unet(\n",
    "        input_shape=(1024, 1024, 3),\n",
    "        mode='cascade',\n",
    "        filters=32,\n",
    "        n_class=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, filters=64, n_block=3, kernel_size=(3, 3), activation='relu'):\n",
    "    skip = []\n",
    "    for i in range(n_block):\n",
    "        print('*********************encoder {}******************'.format(i+1))\n",
    "        print('input {}'.format(x.shape))\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        print('conv 1 {}'.format(x.shape))\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        print('conv 2 {}'.format(x.shape))\n",
    "        skip.append(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "        print('Max {}'.format(x.shape))\n",
    "        print('**************************************************************************')\n",
    "    return x, skip\n",
    "\n",
    "\n",
    "def bottleneck(x, filters_bottleneck, mode='cascade', depth=6,\n",
    "               kernel_size=(3, 3), activation='relu'):\n",
    "    dilated_layers = []\n",
    "    print('*********************************middle*********************************')\n",
    "    print('input {}'.format(x.shape))\n",
    "    if mode == 'cascade':  # used in the competition\n",
    "        print('cascade dilatation')\n",
    "        for i in range(depth):\n",
    "            \n",
    "            x = Conv2D(filters_bottleneck, kernel_size,\n",
    "                       activation=activation, padding='same', dilation_rate=2**i)(x)\n",
    "            dilated_layers.append(x)\n",
    "            print('conv dilatation {} {}'.format(i,x.shape))\n",
    "        return add(dilated_layers)\n",
    "    elif mode == 'parallel':  # Like \"Atrous Spatial Pyramid Pooling\"\n",
    "        print('parallel dilatation')\n",
    "        for i in range(depth):\n",
    "            dilated_layers.append(\n",
    "                Conv2D(filters_bottleneck, kernel_size,\n",
    "                       activation=activation, padding='same', dilation_rate=2**i)(x)\n",
    "            )\n",
    "            print('conv dilatation {} {}'.format(i,Conv2D(filters_bottleneck, kernel_size,\n",
    "                       activation=activation, padding='same', dilation_rate=2**i)(x).shape))\n",
    "        return add(dilated_layers)\n",
    "\n",
    "\n",
    "def decoder(x, skip, filters, n_block=3, kernel_size=(3, 3), activation='relu'):\n",
    "    \n",
    "    \n",
    "    for i in reversed(range(n_block)):\n",
    "        print('*********************************decoder{}*********************************'.format(i+1))\n",
    "        print('input {}'.format(x.shape))\n",
    "#         x = UpSampling2D(size=(2, 2))(x)\n",
    "        print('upsampling {}'.format(x.shape))\n",
    "        print('filters {}'.format(filters * 2**i))\n",
    "        print('kernel {}'.format(kernel_size))\n",
    "        if i == 3 or i == 1:\n",
    "            x = Conv2DTranspose(filters * 2**i, kernel_size, strides=(2, 2), padding=\"same\")(x)\n",
    "# #             x = Conv2D(filters * 2**i, kernel_size, activation=None,strides=(2, 2), padding='valid')(x)\n",
    "# #             x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "#             x = Conv2DTranspose(filters * 2**i, kernel_size, strides=(2, 2), padding=\"same\")(x)\n",
    "        else:\n",
    "            x = Conv2DTranspose(filters * 2**i, kernel_size, strides=(2, 2), padding=\"valid\")(x)\n",
    "#         print('conv 1 {}'.format(x.shape))\n",
    "        print('skip {}'.format(skip[i].shape))\n",
    "        print('conv T {}'.format(x))\n",
    "        x = concatenate([skip[i], x])\n",
    "        print('concat {}'.format(x.shape))\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        print('conv2 {}'.format(x.shape))\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        print('conv3 {}'.format(x.shape))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************encoder 1******************\n",
      "input (?, 101, 101, 1)\n",
      "conv 1 (?, 101, 101, 16)\n",
      "conv 2 (?, 101, 101, 16)\n",
      "Max (?, 50, 50, 16)\n",
      "**************************************************************************\n",
      "*********************encoder 2******************\n",
      "input (?, 50, 50, 16)\n",
      "conv 1 (?, 50, 50, 32)\n",
      "conv 2 (?, 50, 50, 32)\n",
      "Max (?, 25, 25, 32)\n",
      "**************************************************************************\n",
      "*********************encoder 3******************\n",
      "input (?, 25, 25, 32)\n",
      "conv 1 (?, 25, 25, 64)\n",
      "conv 2 (?, 25, 25, 64)\n",
      "Max (?, 12, 12, 64)\n",
      "**************************************************************************\n",
      "*********************encoder 4******************\n",
      "input (?, 12, 12, 64)\n",
      "conv 1 (?, 12, 12, 128)\n",
      "conv 2 (?, 12, 12, 128)\n",
      "Max (?, 6, 6, 128)\n",
      "**************************************************************************\n",
      "*********************************middle*********************************\n",
      "input (?, 6, 6, 128)\n",
      "cascade dilatation\n",
      "conv dilatation 0 (?, 6, 6, 256)\n",
      "conv dilatation 1 (?, 6, 6, 256)\n",
      "conv dilatation 2 (?, 6, 6, 256)\n",
      "conv dilatation 3 (?, 6, 6, 256)\n",
      "conv dilatation 4 (?, 6, 6, 256)\n",
      "conv dilatation 5 (?, 6, 6, 256)\n",
      "*********************************decoder4*********************************\n",
      "input (?, 6, 6, 256)\n",
      "upsampling (?, 6, 6, 256)\n",
      "filters 128\n",
      "kernel (3, 3)\n",
      "skip (?, 12, 12, 128)\n",
      "conv T Tensor(\"conv2d_transpose_9/BiasAdd:0\", shape=(?, ?, ?, 128), dtype=float32)\n",
      "concat (?, 12, 12, 256)\n",
      "conv2 (?, 12, 12, 128)\n",
      "conv3 (?, 12, 12, 128)\n",
      "*********************************decoder3*********************************\n",
      "input (?, 12, 12, 128)\n",
      "upsampling (?, 12, 12, 128)\n",
      "filters 64\n",
      "kernel (3, 3)\n",
      "skip (?, 25, 25, 64)\n",
      "conv T Tensor(\"conv2d_transpose_10/BiasAdd:0\", shape=(?, ?, ?, 64), dtype=float32)\n",
      "concat (?, 25, 25, 128)\n",
      "conv2 (?, 25, 25, 64)\n",
      "conv3 (?, 25, 25, 64)\n",
      "*********************************decoder2*********************************\n",
      "input (?, 25, 25, 64)\n",
      "upsampling (?, 25, 25, 64)\n",
      "filters 32\n",
      "kernel (3, 3)\n",
      "skip (?, 50, 50, 32)\n",
      "conv T Tensor(\"conv2d_transpose_11/BiasAdd:0\", shape=(?, ?, ?, 32), dtype=float32)\n",
      "concat (?, 50, 50, 64)\n",
      "conv2 (?, 50, 50, 32)\n",
      "conv3 (?, 50, 50, 32)\n",
      "*********************************decoder1*********************************\n",
      "input (?, 50, 50, 32)\n",
      "upsampling (?, 50, 50, 32)\n",
      "filters 16\n",
      "kernel (3, 3)\n",
      "skip (?, 101, 101, 16)\n",
      "conv T Tensor(\"conv2d_transpose_12/BiasAdd:0\", shape=(?, ?, ?, 16), dtype=float32)\n",
      "concat (?, 101, 101, 32)\n",
      "conv2 (?, 101, 101, 16)\n",
      "conv3 (?, 101, 101, 16)\n",
      "last (?, 101, 101, 1)\n"
     ]
    }
   ],
   "source": [
    "model1 = get_dilated_unet(\n",
    "        input_shape=(101, 101, 1),\n",
    "        mode='cascade',\n",
    "        filters=16,\n",
    "        n_class=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze_excite_block(input, ratio=16):\n",
    "    init = input\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    filters = init._keras_shape[channel_axis]\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        se = Permute((3, 1, 2))(se)\n",
    "\n",
    "    x = multiply([init, se])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "channel squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, filters=64, n_block=3, kernel_size=(3, 3), activation='relu'):\n",
    "    skip = []\n",
    "    for i in range(n_block):\n",
    "        print('*********************encoder {}******************'.format(i+1))\n",
    "        print('input {}'.format(x.shape))\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        print('conv 1 {}'.format(x.shape))\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        print('conv 2 {}'.format(x.shape))\n",
    "        x = squeeze_excite_block(x, ratio=16)\n",
    "        print('squeeze{}'.format(x.shape))\n",
    "        skip.append(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "        print('Max {}'.format(x.shape))\n",
    "        print('**************************************************************************')\n",
    "    return x, skip\n",
    "\n",
    "\n",
    "def bottleneck(x, filters_bottleneck, mode='cascade', depth=6,\n",
    "               kernel_size=(3, 3), activation='relu'):\n",
    "    dilated_layers = []\n",
    "    print('*********************************middle*********************************')\n",
    "    print('input {}'.format(x.shape))\n",
    "    if mode == 'cascade':  # used in the competition\n",
    "        print('cascade dilatation')\n",
    "        for i in range(depth):\n",
    "            \n",
    "            x = Conv2D(filters_bottleneck, kernel_size,\n",
    "                       activation=activation, padding='same', dilation_rate=2**i)(x)\n",
    "            dilated_layers.append(x)\n",
    "            print('conv dilatation {} {}'.format(i,x.shape))\n",
    "        return add(dilated_layers)\n",
    "    elif mode == 'parallel':  # Like \"Atrous Spatial Pyramid Pooling\"\n",
    "        print('parallel dilatation')\n",
    "        for i in range(depth):\n",
    "            dilated_layers.append(\n",
    "                Conv2D(filters_bottleneck, kernel_size,\n",
    "                       activation=activation, padding='same', dilation_rate=2**i)(x)\n",
    "            )\n",
    "            print('conv dilatation {} {}'.format(i,Conv2D(filters_bottleneck, kernel_size,\n",
    "                       activation=activation, padding='same', dilation_rate=2**i)(x).shape))\n",
    "        return add(dilated_layers)\n",
    "\n",
    "\n",
    "def decoder(x, skip, filters, n_block=3, kernel_size=(3, 3), activation='relu'):\n",
    "    \n",
    "    \n",
    "    for i in reversed(range(n_block)):\n",
    "        print('*********************************decoder{}*********************************'.format(i+1))\n",
    "        print('input {}'.format(x.shape))\n",
    "#         x = UpSampling2D(size=(2, 2))(x)\n",
    "        print('upsampling {}'.format(x.shape))\n",
    "        print('filters {}'.format(filters * 2**i))\n",
    "        print('kernel {}'.format(kernel_size))\n",
    "        if i == 3 or i == 1:\n",
    "            x = Conv2DTranspose(filters * 2**i, kernel_size, strides=(2, 2), padding=\"same\")(x)\n",
    "# #             x = Conv2D(filters * 2**i, kernel_size, activation=None,strides=(2, 2), padding='valid')(x)\n",
    "# #             x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "#             x = Conv2DTranspose(filters * 2**i, kernel_size, strides=(2, 2), padding=\"same\")(x)\n",
    "        else:\n",
    "            x = Conv2DTranspose(filters * 2**i, kernel_size, strides=(2, 2), padding=\"valid\")(x)\n",
    "#         print('conv 1 {}'.format(x.shape))\n",
    "        print('skip {}'.format(skip[i].shape))\n",
    "        print('conv T {}'.format(x))\n",
    "        x = concatenate([skip[i], x])\n",
    "        print('concat {}'.format(x.shape))\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        print('conv2 {}'.format(x.shape))\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        print('conv3 {}'.format(x.shape))\n",
    "        x = squeeze_excite_block(x, ratio=16)\n",
    "        print('squeeze{}'.format(x.shape))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************encoder 1******************\n",
      "input (?, 101, 101, 3)\n",
      "conv 1 (?, 101, 101, 64)\n",
      "conv 2 (?, 101, 101, 64)\n",
      "squeeze(?, 101, 101, 64)\n",
      "Max (?, 50, 50, 64)\n",
      "**************************************************************************\n",
      "*********************encoder 2******************\n",
      "input (?, 50, 50, 64)\n",
      "conv 1 (?, 50, 50, 128)\n",
      "conv 2 (?, 50, 50, 128)\n",
      "squeeze(?, 50, 50, 128)\n",
      "Max (?, 25, 25, 128)\n",
      "**************************************************************************\n",
      "*********************encoder 3******************\n",
      "input (?, 25, 25, 128)\n",
      "conv 1 (?, 25, 25, 256)\n",
      "conv 2 (?, 25, 25, 256)\n",
      "squeeze(?, 25, 25, 256)\n",
      "Max (?, 12, 12, 256)\n",
      "**************************************************************************\n",
      "*********************encoder 4******************\n",
      "input (?, 12, 12, 256)\n",
      "conv 1 (?, 12, 12, 512)\n",
      "conv 2 (?, 12, 12, 512)\n",
      "squeeze(?, 12, 12, 512)\n",
      "Max (?, 6, 6, 512)\n",
      "**************************************************************************\n",
      "*********************************middle*********************************\n",
      "input (?, 6, 6, 512)\n",
      "cascade dilatation\n",
      "conv dilatation 0 (?, 6, 6, 1024)\n",
      "conv dilatation 1 (?, 6, 6, 1024)\n",
      "conv dilatation 2 (?, 6, 6, 1024)\n",
      "conv dilatation 3 (?, 6, 6, 1024)\n",
      "conv dilatation 4 (?, 6, 6, 1024)\n",
      "conv dilatation 5 (?, 6, 6, 1024)\n",
      "*********************************decoder4*********************************\n",
      "input (?, 6, 6, 1024)\n",
      "upsampling (?, 6, 6, 1024)\n",
      "filters 512\n",
      "kernel (3, 3)\n",
      "skip (?, 12, 12, 512)\n",
      "conv T Tensor(\"conv2d_transpose_29/BiasAdd:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "concat (?, 12, 12, 1024)\n",
      "conv2 (?, 12, 12, 512)\n",
      "conv3 (?, 12, 12, 512)\n",
      "squeeze(?, 12, 12, 512)\n",
      "*********************************decoder3*********************************\n",
      "input (?, 12, 12, 512)\n",
      "upsampling (?, 12, 12, 512)\n",
      "filters 256\n",
      "kernel (3, 3)\n",
      "skip (?, 25, 25, 256)\n",
      "conv T Tensor(\"conv2d_transpose_30/BiasAdd:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
      "concat (?, 25, 25, 512)\n",
      "conv2 (?, 25, 25, 256)\n",
      "conv3 (?, 25, 25, 256)\n",
      "squeeze(?, 25, 25, 256)\n",
      "*********************************decoder2*********************************\n",
      "input (?, 25, 25, 256)\n",
      "upsampling (?, 25, 25, 256)\n",
      "filters 128\n",
      "kernel (3, 3)\n",
      "skip (?, 50, 50, 128)\n",
      "conv T Tensor(\"conv2d_transpose_31/BiasAdd:0\", shape=(?, ?, ?, 128), dtype=float32)\n",
      "concat (?, 50, 50, 256)\n",
      "conv2 (?, 50, 50, 128)\n",
      "conv3 (?, 50, 50, 128)\n",
      "squeeze(?, 50, 50, 128)\n",
      "*********************************decoder1*********************************\n",
      "input (?, 50, 50, 128)\n",
      "upsampling (?, 50, 50, 128)\n",
      "filters 64\n",
      "kernel (3, 3)\n",
      "skip (?, 101, 101, 64)\n",
      "conv T Tensor(\"conv2d_transpose_32/BiasAdd:0\", shape=(?, ?, ?, 64), dtype=float32)\n",
      "concat (?, 101, 101, 128)\n",
      "conv2 (?, 101, 101, 64)\n",
      "conv3 (?, 101, 101, 64)\n",
      "squeeze(?, 101, 101, 64)\n",
      "last (?, 101, 101, 1)\n"
     ]
    }
   ],
   "source": [
    "modelcse = get_dilated_unet(\n",
    "        input_shape=(101, 101, 3),\n",
    "        mode='cascade',\n",
    "        filters=16,\n",
    "        n_class=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.fcs = nn.Sequential(nn.Linear(channel, int(channel/reduction)),\n",
    "                                 nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "                                 nn.Linear(int(channel/reduction), channel),\n",
    "                                 nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        bahs, chs, _, _ = x.size()\n",
    "\n",
    "        # Returns a new tensor with the same data as the self tensor but of a different size.\n",
    "        y = self.avg_pool(x).view(bahs, chs)\n",
    "        y = self.fcs(y).view(bahs, chs, 1, 1)\n",
    "        return torch.mul(x, y)\n",
    "\n",
    "\n",
    "class SCSEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SCSEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.channel_excitation = nn.Sequential(nn.Linear(channel, int(channel//reduction)),\n",
    "                                                nn.ReLU(inplace=True),\n",
    "                                                nn.Linear(int(channel//reduction), channel),\n",
    "                                                nn.Sigmoid())\n",
    "\n",
    "        self.spatial_se = nn.Sequential(nn.Conv2d(channel, 1, kernel_size=1,\n",
    "                                                  stride=1, padding=0, bias=False),\n",
    "                                        nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        bahs, chs, _, _ = x.size()\n",
    "\n",
    "        # Returns a new tensor with the same data as the self tensor but of a different size.\n",
    "        chn_se = self.avg_pool(x).view(bahs, chs)\n",
    "        chn_se = self.channel_excitation(chn_se).view(bahs, chs, 1, 1)\n",
    "        chn_se = torch.mul(x, chn_se)\n",
    "\n",
    "        spa_se = self.spatial_se(x)\n",
    "        spa_se = torch.mul(x, spa_se)\n",
    "        return torch.add(chn_se, 1, spa_se)\n",
    "\n",
    "\n",
    "class ModifiedSCSEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(ModifiedSCSEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.channel_excitation = nn.Sequential(nn.Linear(channel, int(channel//reduction)),\n",
    "                                                nn.ReLU(inplace=True),\n",
    "                                                nn.Linear(int(channel//reduction), channel),\n",
    "                                                nn.Sigmoid())\n",
    "\n",
    "        self.spatial_se = nn.Sequential(nn.Conv2d(channel, 1, kernel_size=1,\n",
    "                                                  stride=1, padding=0, bias=False),\n",
    "                                        nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        bahs, chs, _, _ = x.size()\n",
    "\n",
    "        # Returns a new tensor with the same data as the self tensor but of a different size.\n",
    "        chn_se = self.avg_pool(x).view(bahs, chs)\n",
    "        chn_se = self.channel_excitation(chn_se).view(bahs, chs, 1, 1)\n",
    "\n",
    "        spa_se = self.spatial_se(x)\n",
    "return torch.mul(torch.mul(x, chn_se), spa_se)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
